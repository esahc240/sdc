{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning?\n",
    "Input layer - inputs\n",
    "Output layers - answering your question\n",
    "inbetween is the \"hidden layer\" - billions and billions of neurons that process information\n",
    "\n",
    "Where is the deep learning?\n",
    "Separate the layers further to include more and more hidden layers - then connect everything - other hidden layers serve as inputs to hidden layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Plan of attack\n",
    "- Neuron\n",
    "- Activation Function - which one is best for you, which layers you want to use functions in\n",
    "- How do neural networks work? - Seeing a neural network in action will give us a goal\n",
    "- How do neural networks learn?\n",
    "- Gradient Descent - how it's better than a \"brute force\" method we always use, idiots\n",
    "- Stochastic Gradient Descent - even better/stronger than the lame-ass predecessor\n",
    "- Backpropagation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neuron\n",
    "The basic building block of deep learning\n",
    "Start with the neuron - gets an input and gives an output\n",
    "- Inputs are represented by other neurons\n",
    " - yellow means an input layer\n",
    " - sometimes yellows get inputs from other yellows\n",
    "- the inputs are your senses - hear touch smell etc.\n",
    "- Your brain is locked in a black box - we have no idea what the heck is going on in there, tastes go in and we taste etc.\n",
    "- Signals from inputs are passed through synapses, outputs are passed through other synapses\n",
    "\n",
    "Lets look at the different elements\n",
    "#### Input layer\n",
    "- Inputs are independent variables\n",
    " - one row in your database, age, money, etc.\n",
    "- you need to standardize (mean 0 stddev 1) all your inputs, or normarlize them, which one? We'll find out later, calm down\n",
    " - you basically want all of them to be within simular ranges of values- they'll all be combined etc, so it's easier for processing if there's some homogeny\n",
    " - good additional reading is http://yann.lecun.com/exdb/bulis/pdf/lecun-98b.pdf for standardization and normalization\n",
    "- Output values can be continuous (price) binary (person chose to stay or not) or categorical (one of several values)- need to represent these as numbers\n",
    "Whatever inputs you put in is for one row, the output is that one row - like a multivariate regression, it's always one row per time in a neuron with multiple columns\n",
    "\n",
    "#### Synapses\n",
    "- all assigned weights\n",
    "- weighting is crucial - it's how they learn\n",
    " - by adjusting the weights the neuron can judge which are most important\n",
    " - these are what get adjusted by the neuron - gradient descent/backpropagation\n",
    "\n",
    "#### Neuron\n",
    "- what happens in the neuron?\n",
    " - all of the values get added up, it take sthe weighted sum of all the input values\n",
    " - add up, multiply by weight, add it up etc.\n",
    "- Then it applys an activation function\n",
    " - a function assigned to the layer\n",
    " - applied to the weighted sum, neuron passes that signal on as the output depending on the function (sometimes it won't pass on the signal from the function?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Activation Function\n",
    "\n",
    "What options do we have for the activation function?\n",
    "\n",
    "#### Threshold function\n",
    "On the x axis you have the weighted inputs, on the y you have a value from 0 to 1. \n",
    "The threshold function is a smple function, if the value is less than 0 it passes on a 0, if it's greater than 0 than passes on a 1.\n",
    "\n",
    "#### Sigmoid function\n",
    "$$ 1/1+e^-x $$\n",
    "It's a function used in logistic regression - what's good about it is it's smooth, unlike the threshold function. Anything below 0 drops off to 0, anything above zero approximates to 1. Helpful for predicting probability\n",
    "\n",
    "#### Rectifier function\n",
    "most popular for ANN. Anything below 0 is 0 and it moves linearly to 1\n",
    "\n",
    "#### Hyperbolic tangent function\n",
    "Goes below 0. From -1 below 0 to +1 above 0\n",
    "\n",
    "#### two quick exercises\n",
    "if you have a binary input, might as well use threshold function?\n",
    "- two options\n",
    " - threshold - got it\n",
    " - could also use sigmoid, because the input between 0 and 1, and you want a 0 and 1. Good for predicting probility - will give us the probabili8ty that y = 1\n",
    "How would it play out if we had multiple inputs and 4 functions in the hidden layer\n",
    "- in the hidden layers apply the rectifier function, then the sigmoid in the output function. Why rectifier?\n",
    "#### Each input goes to each function in the hidden layer\n",
    "\n",
    "-------\n",
    "\n",
    "Additional Reading - deep sparce rectifier neural networks. Will tell you why the rectifier function is so popular.\n",
    "link: http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do neural networks work?\n",
    "\n",
    "Working step by step through the process\n",
    "- Property valuation is the tutorial example\n",
    " - we won't look at training, which you'll need to do normally, but we'll be looking at just the application of the network\n",
    " \n",
    "Lets say we have some input parameters\n",
    "- area sqft\n",
    "- bedrooms\n",
    "- distance to city\n",
    "- age\n",
    "\n",
    "In it's basic form, the input and output only exist, no hidden layers. The inputs are weighted and the output function decides the price\n",
    "- you can use basically any of the functions we talked about before\n",
    "- goes to show how powerful nn's are, even without the layers it's powerful, but the bonus of nn is the additional layers and the power they give\n",
    "\n",
    "Now we're going to understand how the layer gives the extra power\n",
    "#### Walkthrough of inputs and hidden layers\n",
    "\n",
    "With all 4 variables on the left, we'll run them all through the first of the 5 hidden layer functions.\n",
    "- there are synapses that connect this info to the activation function\n",
    " - With a specific neuron, we know that some inputs are not important, and some are\n",
    " - how do we know this? The neuron (activation func) - from their distance from the city, they have an abnormally high area, the neuron might be picking out those specific properties. It will only fire if the area is large and distance is away from the city. It's focussed?\n",
    "- Looking at the middle neuron - we have 3 inputs pointing to this neuron in the hidden layer\n",
    " - why does this neuron only focus on these three? It knows that the combination of area/bedrooms/age are important because in its training it found that these properties\n",
    " - maybe there's a segment of the population that prefers a certain combination of these three inputs\n",
    " - it combines these values into a new attribute to serve as an input to the output neuron\n",
    "- the last HL neuron activation function prefers only age as an input\n",
    " - it may have identified a correlation with an age threshold and price (very old houses are highly valued)\n",
    " - as soon as it sees a property over 100yrs old it will fire up\n",
    " - Good example of the rectifier function. It's zero until it reaches an age (0), and then it escalates in price as the age increases\n",
    "- The network can pick up things that we wouldn't pick up either\n",
    " - combinations of all 4 may exist if there's some correlation that exists\n",
    "#### The power of the network is that it's flexible and can apply several predictions in one model that are weighted to create the output. Each activation function neuron in the hidden layer can have a threshold to be reached before it \"fires\" to create an output value. The output is scaled once past the threshold to provide more weight, etc. Looks like the hyperbolic tangent function can provide negative weight as well - interesting\n",
    "- How to decide the number of activation functions/neurons in the hidden layer\n",
    "- how to decide which activation functions to use for each neuron?\n",
    "- how do the neurons not duplicate their results/purpose? Do you prevent that by only having one activation function of each type?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do Neural Networks Learn?\n",
    "\n",
    "Two fundamentally different approaches to get a program to do what you want to do:\n",
    "- hardcoded programming, account for every possible scenario\n",
    "- neural networks, you create the facility for the network to decide what to do on its own\n",
    "Two different approaches - our goal is to create the network to learn on its own - avoid putting in rules on our own\n",
    "###### for instance, how do you distinguish between a dog and a cat - hardcoded means look at nose, shapes, face, colors, etc. describe everything or it fails/sucks. For a neural network, you create the architechure, and then point it at a set of images for cats and dogs that are categorized, and the nn finds out what it needs to understand on its own.\n",
    "\n",
    "WIth a single layer feed-forward NN (also known as a perceptron). Usually y stands for the actual value, y-hat is the predicted value from the NN\n",
    "\n",
    "#### How does a perceptron learn?\n",
    "- lets give some input, the activation function is applied, then we get the output y-hat, then compare to the actual\n",
    "- Lets calculate the cost function = 1half the square of the difference of y-hat minus y. AKA what's the error in your prediction - the lower the cost the lower the y-hat is to y\n",
    "- The cost function is then fed back to the network and the weights are adjusted\n",
    " - all we have control of is the weights\n",
    "- we update the weights, we'll find out how later\n",
    "- We're feeding in the same values (row) over and over, the cost function is fed back in, and the weights are adjusted, the weights continue to change until the cost function is zero\n",
    "\n",
    "#### How does it work with multip;le rows?\n",
    "Each row feeds the same network. \n",
    "One epoch is when we go through the whole dataset and train the network.\n",
    "Once an epoch is completed, the cost function is a sum of all the differences for each row.\n",
    "The weights are updated with the cost function. Weights are the same for each row of data, which is why we use the sum of the cost functions rather than calculate them differently.\n",
    "When the cost function reaches the minimum the model is complete\n",
    "This is called backpropagation\n",
    "Further reading on the cost function: http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Backpropagation is when the cost function value is put back into the network and the weights are adjusted. Now we're learn how those weights are adjusted.\n",
    "\n",
    "Looking at a simpler network with 1 input, 1 af neuron, and one output. \n",
    "\n",
    "#### How do we calculate cost\n",
    "With brute force, you end up with a parabolic function. We know the best spot is the minimum. If you have one weight, it might work, but with more complexity it sucks.\n",
    "#### Curse of dementionality\n",
    "Before the nn is trained, every input goes to every neuron in the HL, then to the output (25 synapses in the previous example). How would we brute force our way through that one. There are 25 weights, 1000 combinations, so 1000 to the 25th power. It would take 10^50 years.\n",
    "\n",
    "#### Gradient descent\n",
    "Given a parabolic cost function, what's a faster way to find the minimum?\n",
    "- start at a random part, get the angle of the cost function at that point\n",
    " - Get the slope, is it positive or negative\n",
    " - take another step based on the angle of the slope\n",
    "- Imagine a ball rolling on the slope, it will keep going back and forth until it reaches the minimum\n",
    "###### How do we know the minimum?\n",
    "\toh, the cost function is parabolic, so there will always be a minmimum. Instead of constructing the line by brute force, use gradient descent to guess-and-check your way through it. awesome\n",
    "    \n",
    "###### How does the NN know which way to adjust the weighting of the synapses in order to adjust the cost function point closer to the minimum instead of just anywhere? If it keeps getting progressivly better negative slopes, does it know to jump ahead more so it doesn't take 1000 tiny steps relative to the overall parabola and get nowhere near the minimum?\n",
    "\n",
    "Called gradient descent because you're descending to the minimum. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent - efficient when minimizing the cost function, 10^57 yrs to hours\n",
    "- however, it requires the cost function to be convex\n",
    "\n",
    "#### What if it _isn't_ convex?\n",
    "Multi-dimension might result in something that isn't convex\n",
    "With gradient descent, it will find the local minimum possibly. \n",
    "\n",
    "#### Stochasitc vs Batch Gradient\n",
    "Gradient takes all the rows and plug them in, calculate cost, readjust the weights, etc.\n",
    "Stochastic takes rows one by one, adjusting the weights after each input row. \n",
    "###### why isn't this slower?\n",
    "- turns out it's faster because it doesn't load all the data into memory and store the results.\n",
    "The stochastic method removes the problem of local minimums rather than global minimum. \n",
    "GD is a deterministic function rather than stochastic. That means GD uses the same goal weights for each epoch, while stochastic's goal weights will change with each row.\n",
    "\n",
    "###### How does stoch find the minimum? by chance?\n",
    "\n",
    "#### Mini-batch gradient descent is the combo of the two, where you do stochastic sampling of smaller batches. \n",
    "Additional reading: https://iamtrask.github.io/2015/07/27/python-network-part2\n",
    "- more math on gradient descent (do this): http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "We calculate errors and backpropogate the changes through the network.\n",
    "All weights are adjusted simultaneously. This is the advantage of BP - because of the way the alg is structured, you know what each weighting will change in your error rate.\n",
    "Additional reading above (nnanddL.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step walkthrough\n",
    "Step 1: Randomly initialise the weights to small numbers close to 0 but not 0\n",
    "\n",
    "Step 2: input the first observation of your dataset in the input layer, each feature in one input note. \n",
    "\n",
    "Step 3: Forward-Propagation: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights (the weights basically determine how important each imput is). Propagate the activations until getting the predicted result y-hat.\n",
    "\n",
    "Step 4: Compare the predicted result to the actual result. Measure the generated error.\n",
    "\n",
    "Step 5: Back-propagation: from right to left, the error is back-propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.\n",
    "\n",
    "Step 6: Repeate steps 1 to 5 and update the weights after each observation (Reinforcement Learning (Stochasitc)). Or: Repeat steps 1 to 5 but update the weights only after a batch of observations (Batch Learning (Batch Gradient Descent)).\n",
    "\n",
    "Step 7: When the whole training set passed through the ANN, that makes an epoch. Redo more epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
